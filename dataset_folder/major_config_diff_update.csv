Owner,main_model_name,to_compare,largefile,card,key,main_value,compared_value
Ahmed007,Ahmed007/Copilot_for_poors_v2,Ahmed007/Copilot_for_poors,Different,Different,_name_or_path,Ahmed007/Copilot_for_poors,t5-base
albert,albert/albert-xlarge-v1,albert/albert-xlarge-v2,Different,Different,"attention_probs_dropout_prob, hidden_act, hidden_dropout_prob","0.1, gelu, 0.1","0, gelu_new, 0"
alpcansoydas,alpcansoydas/bert-base-arabic-emotion-analysis-v2,alpcansoydas/bert-base-arabic-emotion-analysis,Different,Different,transformers_version,4.35.2,4.35.1
AmelieSchreiber,AmelieSchreiber/esm2_t6_8M_general_binding_sites_v2,AmelieSchreiber/esm2_t6_8M_general_binding_sites,Different,Different,no-changes,no-changes,no-changes
ankhamun,ankhamun/x_x-v0,ankhamun/xxxI0v_v0Ixxx,Different,Different,"_name_or_path, sliding_window, transformers_version, use_cache","./yellow, 4096, 4.36.2, True","./finaloutputmodel, None, 4.38.0.dev0, False"
arjan-hada,arjan-hada/esm2_t12_35M_UR50D-finetuned-rep7868aav2-v0,arjan-hada/esm2_t12_35M_UR50D-finetuned-rep7868aav2-v1,Different,Different,no-changes,no-changes,no-changes
athirdpath,athirdpath/Iambe-20b-DARE-v2,athirdpath/Iambe-20b-DARE,Different,Different,"_name_or_path, use_cache","athirdpath/BigLlama-20b-v1.1, True","athirdpath/BigLlama-20B, False"
austinpatrickm,austinpatrickm/finetuned_bge_embeddings_v2,austinpatrickm/finetuned-bge-embeddings,Different,Different,transformers_version,4.35.2,4.33.1
bedust,bedust/tinyllama-colorist-v2,bedust/tinyllama-colorist-lora,Different,Different,no-information,no-information,no-information
Biomimicry-AI,Biomimicry-AI/ANIMA-Nectar-v2,Biomimicry-AI/ANIMA-Nectar-v3-GGUF,Different,Different,no-information,no-information,no-information
chathuranga-jayanath,chathuranga-jayanath/codet5-base-v1,chathuranga-jayanath/codet5-base-v2,Different,Different,no-changes,no-changes,no-changes
chensyii,chensyii/my_bert_model_v2,chensyii/my_bert_model,Different,Different,transformers_version,4.35.2,4.34.1
ClaudiaRichard,ClaudiaRichard/mbti-bert-nli-finetuned_v2,ClaudiaRichard/mbti-bert-nli-finetuned,Different,Different,no-changes,no-changes,no-changes
Conrad747,Conrad747/luganda-ner-v6,Conrad747/luganda-ner-v5,Different,Different,"_name_or_path, hidden_size, intermediate_size, num_attention_heads, num_hidden_layers, transformers_version, adapters, gradient_checkpointing","xlm-roberta-base, 768, 3072, 12, 12, 4.35.2, None, None","masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0, 1024, 4096, 16, 24, 4.27.4, {'adapters': {}, 'config_map': {}, 'fusion_config_map': {}, 'fusions': {}}, False"
csarron,csarron/mobilebert-uncased-squad-v2,csarron/mobilebert-uncased-squad-v1,Different,Different,no-changes,no-changes,no-changes
csarron,csarron/mobilebert-uncased-squad-v2,csarron/mobilebert-uncased-squad-v1,Different,Different,no-changes,no-changes,no-changes
cuadron11,cuadron11/suicide-distilbert-original-8-5-v2,cuadron11/suicide-distilbert-original-8-5,Different,Different,no-changes,no-changes,no-changes
damerajee,damerajee/tinyllama-sft-small-v2,damerajee/Tinyllama-sft-small,Different,Different,no-information,no-information,no-information
decapoda-research,decapoda-research/Antares-11b-v2,decapoda-research/Antares-11b-v1,Different,Different,"_name_or_path, transformers_version","decapoda-research/Antares-11b-v1, 4.37.2","merged, 4.36.2"
evenicole,evenicole/google-play-sentiment-analysis_v2,evenicole/google-play-sentiment-analysis,Different,Different,no-changes,no-changes,no-changes
Fizzarolli,Fizzarolli/sappha-2b-v1,Fizzarolli/sappha-2b-v3,Different,Different,"_name_or_path, torch_dtype, transformers_version, unsloth_version, hidden_activation","unsloth/gemma-2b-bnb-4bit, bfloat16, 4.38.1, 2024.2, None","unsloth/gemma-2b, float32, 4.39.1, None, None"
Frrrrrrrrank,Frrrrrrrrank/Llama-2-7b-chat-hf-process_engineering_one_firsttwokap_v3,Frrrrrrrrank/Llama-2-7b-chat-hf-process_engineering_one_firsttwokap_v2,Different,Different,no-information,no-information,no-information
gangyeolkim,gangyeolkim/kobart-korean-summarizer-v2,gangyeolkim/open-llama-2-ko-7b-summarization,Different,Different,"_name_or_path, activation_dropout, activation_function, add_bias_logits, add_final_layer_norm, architectures, author, classif_dropout, classifier_dropout, d_model, decoder_attention_heads, decoder_ffn_dim, decoder_layerdrop, decoder_layers, decoder_start_token_id, do_blenderbot_90_layernorm, dropout, early_stopping, encoder_attention_heads, encoder_ffn_dim, encoder_layerdrop, encoder_layers, eos_token_id, extra_pos_embeddings, force_bos_token_to_be_generated, forced_eos_token_id, gradient_checkpointing, id2label, init_std, is_encoder_decoder, kobart_version, label2id, length_penalty, max_length, max_position_embeddings, min_length, model_type, normalize_before, normalize_embedding, num_hidden_layers, pad_token_id, scale_embedding, static_position_embeddings, tokenizer_class, transformers_version, vocab_size, attention_bias, hidden_act, hidden_size, initializer_range, intermediate_size, num_attention_heads, num_key_value_heads, pretraining_tp, rms_norm_eps, rope_scaling, rope_theta, tie_word_embeddings","../notebooks/results/korean-total-merged/checkpoint-34000, 0.0, gelu, False, False, ['BartForConditionalGeneration'], Gangyeol Kim(gangyeol.kim@lotte.net), 0.1, 0.1, 768, 16, 3072, 0.0, 6, 1, False, 0.1, True, 16, 3072, 0.0, 6, [1700, 230], 2, False, 1, False, {'0': 'NEGATIVE', '1': 'POSITIVE'}, 0.02, True, 2.0, {'NEGATIVE': 0, 'POSITIVE': 1}, 2.0, 1024, 1026, 30, bart, False, True, 6, 3, False, False, PreTrainedTokenizerFast, 4.36.0.dev0, 30000, None, None, None, None, None, None, None, None, None, None, None, None","./result/beomi/open-llama-2-ko-7b-summarization, None, None, None, None, ['LlamaForCausalLM'], None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 2, None, None, None, None, None, None, None, None, None, None, None, 2048, None, llama, None, None, 32, 2, None, None, None, 4.38.2, 46336, False, silu, 4096, 0.02, 11008, 32, 32, 1, 1e-06, None, 10000.0, False"
genify-official,genify-official/genify-llama-2-7B-32K-Instruct-Q4-v2,genify-official/genify-llama-2-7B-32K-Instruct-Q4,Different,Different,"attention_bias, attention_dropout, rope_theta, transformers_version","False, 0.0, 10000.0, 4.38.2","None, None, None, 4.31.0"
Gnartiel,Gnartiel/multi-sbert-v2,Gnartiel/multi-sbert,Different,Different,transformers_version,4.35.0,4.34.1
GRPUI,GRPUI/sgugit-model-v3,GRPUI/sgugit-model,Different,Different,no-information,no-information,no-information
Guilherme34,Guilherme34/SamanthaCode-v1,Guilherme34/SamanthaCode,Different,Different,no-information,no-information,no-information
Hejjee,Hejjee/iDonna_depression_v2,Hejjee/iDonna_depression_v1,Different,Different,no-information,no-information,no-information
hoanghoavienvo,hoanghoavienvo/roberta-base-detect-cheapfake-co1-co2-v2,hoanghoavienvo/roberta-base-detect-cheapfake-co1-co2,Different,Different,no-changes,no-changes,no-changes
iagoalves,iagoalves/sentiment-model-v2,iagoalves/sentiment-model-v3,Different,Different,no-information,no-information,no-information
IkariDev,IkariDev/Athena-v2,IkariDev/Athena-v1,Different,Different,"_name_or_path, rope_theta, torch_dtype, transformers_version","Undi95/AthenaV2part1, 10000.0, float32, 4.33.2","IkariDev/Athena-tmpv2, None, float16, 4.32.1"
intelliwork,intelliwork/Llama-2-7b-chat-hf-function-calling-v2,intelliwork/Llama-2-7b-chat-hf-function-calling-adapters-v2,Different,Different,no-information,no-information,no-information
intfloat,intfloat/e5-base-v2,intfloat/e5-base,Different,Different,transformers_version,4.29.0.dev0,4.15.0
Ja-ck,Ja-ck/Mistral-instruct-Y24-v5,Ja-ck/Mistral-instruct-Y24-v6,Different,Different,"_name_or_path, eos_token_id, vocab_size","mistralai/Mistral-7B-v0.1, 2, 32000","/models/Mistral-7B-v0.1/, 32001, 32002"
Jairnetojp,Jairnetojp/hate-classification-distilbert-base-multilingual-cased-sentiments-student-v2,Jairnetojp/hate-classification-distilbert-base-multilingual-cased-sentiments-student,Different,Different,"_name_or_path, transformers_version","Jairnetojp/hate-classification-distilbert-base-multilingual-cased-sentiments-student-v2, 4.38.1","lxyuan/distilbert-base-multilingual-cased-sentiments-student, 4.34.0"
JoaoJunior,JoaoJunior/T5_APR_java_python_v4,JoaoJunior/T5_APR_java_python_v3,Different,Different,no-changes,no-changes,no-changes
Joshua-Abok,Joshua-Abok/flan-t5-large-dialogsum_samsum_v2,Joshua-Abok/flan-t5-large-dialogsum_samsum_v1,Different,Different,no-information,no-information,no-information
jsfs11,jsfs11/MixtureofMerges-MoE-v2,jsfs11/MixtureofMerges-MoE-4x7b-v3,Different,Different,"_name_or_path, torch_dtype","CultriX/Wernicke-7B-v9, float16","senseable/WestLake-7B-v2, bfloat16"
KnutJaegersberg,KnutJaegersberg/megatron-gpt2-345m-evol_instruct_v2,KnutJaegersberg/megatron-GPT-2-345m-EvolInstruct,Different,Different,"_name_or_path, transformers_version, use_cache","/run/media/knut/HD/huggingface models/language models/gpt/megatron-gpt2-345m/, 4.32.0.dev0, False","robowaifudev/megatron-gpt2-345m, 4.29.0.dev0, True"
krevas,krevas/LDCC-Instruct-Llama-2-ko-13B-v4,krevas/LDCC-Instruct-Llama-2-ko-13B-v4.1,Different,Different,no-changes,no-changes,no-changes
martyn,martyn/mixtral-dare-8x7b-v0,martyn/mixtral-megamerge-dare-8x7b-v1,Different,Different,"_name_or_path, output_router_logits, sliding_window, transformers_version, use_cache","mistralai/Mixtral-8x7B-v0.1, False, 4096, 4.36.0.dev0, True","/workspace/models/Mixtral-8x7B-v0.1, True, None, 4.37.0.dev0, False"
MAsad789565,MAsad789565/GPT2_Finetuned_v1,MAsad789565/GPT2_Finetuned,Different,Different,no-information,no-information,no-information
meetkai,meetkai/functionary-7b-v2,meetkai/functionary-7b-v2.1,Different,Different,"_name_or_path, sliding_window, transformers_version","Mistral-7B-v0.1, 4096, 4.35.0","mistralai/Mistral-7B-v0.1, 8192, 4.35.2"
mjkmain,mjkmain/gpt2-imdb-pos-v2,lvwerra/gpt2-imdb-pos,Different,Different,"_name_or_path, architectures, n_inner, reorder_and_upcast_attn, scale_attn_by_inverse_layer_idx, scale_attn_weights, torch_dtype, transformers_version, use_cache, _num_labels, id2label, label2id","lvwerra/gpt2-imdb, ['GPT2LMHeadModel'], None, False, False, True, float32, 4.37.2, True, None, None, None","None, ['GPT2HeadWithValueModel'], None, None, None, None, None, None, None, 1, {'0': 'LABEL_0'}, {'LABEL_0': 0}"
moneyforward,moneyforward/houou-instruction-7b-v2,moneyforward/houou-instruction-7b-v1,Different,Different,no-information,no-information,no-information
moriire,moriire/medical-chat-v0,moriire/phi-2-medical-chat-v0-model,Different,Different,"_name_or_path, architectures, attention_bias, bos_token_id, eos_token_id, hidden_act, hidden_size, intermediate_size, model_type, num_hidden_layers, num_key_value_heads, pretraining_tp, rms_norm_eps, transformers_version, vocab_size, attention_dropout, auto_map, embd_pdrop, layer_norm_eps, partial_rotary_factor, qk_layernorm, resid_pdrop","TinyLlama/TinyLlama-1.1B-Chat-v1.0, ['LlamaForCausalLM'], False, 1, 2, silu, 2048, 5632, llama, 22, 4, 1, 1e-05, 4.35.0, 32000, None, None, None, None, None, None, None","microsoft/phi-2, ['PhiForCausalLM'], None, 50256, 50256, gelu_new, 2560, 10240, phi, 32, 32, None, None, 4.37.2, 50295, 0.0, {'AutoConfig': 'microsoft/phi-2--configuration_phi.PhiConfig', 'AutoModelForCausalLM': 'microsoft/phi-2--modeling_phi.PhiForCausalLM'}, 0.0, 1e-05, 0.4, False, 0.1"
mrm8488,mrm8488/chEMBL_smiles_v1,mrm8488/chEMBL26_smiles_v2,Different,Different,"attention_probs_dropout_prob, vocab_size","0.1, 1784","0.2, 2229"
NbAiLab,NbAiLab/nb-gpt-j-6B-v2,NbAiLab/nb-gpt-j-6B,Different,Different,no-information,no-information,no-information
nchen909,nchen909/codellm-7b-v4,nchen909/codellm-7b-v5,Different,Different,"_name_or_path, transformers_version, attention_dropout","../../deepseek-coder-6.7b-instruct, 4.35.0, None","ckpt_180k_code_data, 4.37.2, 0.0"
ncsgobubble,ncsgobubble/Llama-7B-rollercoaster_v2,ncsgobubble/Llama-7B-rollercoaster,Different,Different,no-information,no-information,no-information
netoferraz,netoferraz/distilbert-base-uncased-finetuned-pad-clf-v2,netoferraz/distilbert-base-uncased-finetuned-pad-clf-v1,Different,Different,no-information,no-information,no-information
NikitaKukuzey,NikitaKukuzey/Lomonosov_small_v1,NikitaKukuzey/Lomonosov_small_v2,Different,Different,"_name_or_path, architectures, d_ff, dense_act_fn, feed_forward_proj, is_gated_act, model_type, n_positions, num_decoder_layers, num_heads, num_layers, output_past, task_specific_params, vocab_size, tie_word_embeddings, tokenizer_class","t5-small, ['T5ForConditionalGeneration'], 2048, relu, relu, False, t5, 512, 6, 8, 6, True, {'summarization': {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 200, 'min_length': 30, 'no_repeat_ngram_size': 3, 'num_beams': 4, 'prefix': 'summarize: '}, 'translation_en_to_de': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to German: '}, 'translation_en_to_fr': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to French: '}, 'translation_en_to_ro': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to Romanian: '}}, 32128, None, None","google/mt5-small, ['MT5ForConditionalGeneration'], 1024, gelu_new, gated-gelu, True, mt5, None, 8, 6, 8, None, None, 250112, False, T5Tokenizer"
nomic-ai,nomic-ai/nomic-embed-text-v1,nomic-ai/nomic-embed-text-v1.5,Different,Different,"rotary_scaling_factor, summary_first_dropout, transformers_version, max_trained_positions","2, 0.1, 4.34.0, None","None, 0.0, 4.37.2, 2048"
ohwi,ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0,ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1,Different,Different,_name_or_path,stabilityai/japanese-stablelm-instruct-gamma-7b,/mnt/disks/disk-33/hwigeon/projects/notus/v1/gamma/results/gamma-sft-full-trial14
Oillim,Oillim/MiniLM-L6-v2,Oillim/sts_all-MiniLM-L6-v2,Different,Different,no-information,no-information,no-information
oshizo,oshizo/japanese-sexual-moderation-v2,oshizo/japanese-sexual-moderation,Different,Different,"id2label, label2id, problem_type, transformers_version","{'0': 'LABEL_0'}, {'LABEL_0': 0}, regression, 4.36.1","{'0': 'strength'}, {'strength': 0}, multi_label_classification, 4.33.0.dev0"
plgrm720,plgrm720/tokipona_to_eng_model_v1,plgrm720/tokipona_to_eng_model_v1.1,Different,Different,no-changes,no-changes,no-changes
Prasadrao,Prasadrao/xlm-roberta-large-go-emotions-v3,Prasadrao/xlm-roberta-large-go-emotions-v2,Different,Different,_name_or_path,Prasadrao/xlm-roberta-large-go-emotions-v2,Prasadrao/xlm-roberta-large-go-emotions-v1
qnguyen3,qnguyen3/quan-1.8b-base-v2,qnguyen3/quan-1.8b-base,Different,Different,"_name_or_path, attention_dropout, bos_token_id, transformers_version","qnguyen3/quan-1.8b-1e, 0.0, 128245, 4.37.0","KnutJaegersberg/Qwen-1_8B-Llamafied, None, 151643, 4.34.1"
recoilme,recoilme/insomnia_v1,recoilme/insomnia_v2,Different,Different,_name_or_path,openai-community/gpt2,recoilme/insomnia_v1
rhplus0831,rhplus0831/maid-yuzu-v1,rhplus0831/maid-yuzu-v2,Different,Different,_name_or_path,mistralai/Mixtral-8x7B-v0.1,smelborp/MixtralOrochi8x7B
rhysjones,rhysjones/phi-2-orange-v2,rhysjones/phi-2-orange,Different,Different,"_name_or_path, attention_dropout, bos_token_id, eos_token_id, hidden_act, hidden_size, intermediate_size, layer_norm_eps, max_position_embeddings, model_type, num_attention_heads, num_hidden_layers, num_key_value_heads, partial_rotary_factor, qk_layernorm, rope_scaling, rope_theta, torch_dtype, transformers_version, use_cache, activation_function, attn_pdrop, auto_map, flash_attn, flash_rotary, fused_dense, img_processor, layer_norm_epsilon, n_embd, n_head, n_head_kv, n_inner, n_layer, n_positions, rotary_dim","rhysjones/phi-2-orange-v2, 0.0, 50256, 50295, gelu_new, 2560, 10240, 1e-05, 2048, phi, 32, 32, 32, 0.4, False, None, 10000.0, float16, 4.37.0, True, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None","phi2-orange-sft, None, None, None, None, None, None, None, None, phi-msft, None, None, None, None, None, None, None, bfloat16, 4.37.0.dev0, False, gelu_new, 0.0, {'AutoConfig': 'configuration_phi.PhiConfig', 'AutoModelForCausalLM': 'modeling_phi.PhiForCausalLM'}, False, False, False, None, 1e-05, 2560, 32, None, None, 32, 2048, 32"
saileshaman,saileshaman/t5-small-finetuned-dialogsum-v3,saileshaman/t5-small-finetuned-dialogsum-v2,Different,Different,no-information,no-information,no-information
Shakhovak,Shakhovak/flan-t5-base-sheldon-chat-v2,Shakhovak/flan-t5-base-sheldon-chat,Different,Different,no-changes,no-changes,no-changes
ShynBui,ShynBui/comment_classification_v2,ShynBui/comment_classification,Different,Different,no-information,no-information,no-information
sjrhuschlee,sjrhuschlee/deberta-v3-base-squad2-ext-v1,sjrhuschlee/deberta-v3-base-squad2,Different,Different,transformers_version,4.31.0.dev0,4.30.0.dev0
stabilityai,stabilityai/stablelm-base-alpha-3b-v2,stabilityai/stablelm-base-alpha-3b,Different,Different,"architectures, auto_map, hidden_act, hidden_size, model_type, norm_eps, num_heads, num_hidden_layers, rotary_scaling_factor, transformers_version, vocab_size, _name_or_path, intermediate_size, layer_norm_eps, num_attention_heads, use_parallel_residual","['StableLMAlphaForCausalLM'], {'AutoConfig': 'configuration_stablelm_alpha.StableLMAlphaConfig', 'AutoModelForCausalLM': 'modeling_stablelm_alpha.StableLMAlphaForCausalLM'}, silu, 2560, stablelm_alpha, 1e-05, 32, 32, 1.0, 4.30.2, 50432, None, None, None, None, None","['GPTNeoXForCausalLM'], None, gelu, 4096, gpt_neox, None, None, 16, None, 4.27.4, 50688, /fsx/ckpts/3b_tok=neox_data=pilev2-recontam=p3_lower-code_bs=8m_tp=4_pp=1_init=wang-small-init/global_step84000_hf, 16384, 1e-05, 32, True"
surprisedPikachu007,surprisedPikachu007/search_summarize_v1,surprisedPikachu007/mt5-small-search-summarizer,Different,Different,"_name_or_path, architectures, d_ff, dense_act_fn, feed_forward_proj, is_gated_act, model_type, n_positions, num_decoder_layers, num_heads, num_layers, output_past, task_specific_params, vocab_size, tie_word_embeddings, tokenizer_class","t5-small, ['T5ForConditionalGeneration'], 2048, relu, relu, False, t5, 512, 6, 8, 6, True, {'summarization': {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 200, 'min_length': 30, 'no_repeat_ngram_size': 3, 'num_beams': 4, 'prefix': 'summarize: '}, 'translation_en_to_de': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to German: '}, 'translation_en_to_fr': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to French: '}, 'translation_en_to_ro': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to Romanian: '}}, 32128, None, None","google/mt5-small, ['MT5ForConditionalGeneration'], 1024, gelu_new, gated-gelu, True, mt5, None, 8, 6, 8, None, None, 250112, False, T5Tokenizer"
Swarnava,Swarnava/T5_base_title_v2,Swarnava/T5_base_title,Different,Different,no-changes,no-changes,no-changes
T-Systems-onsite,T-Systems-onsite/mt5-small-sum-de-en-v2,deutsche-telekom/mt5-small-sum-de-en-v1,Different,Different,"torch_dtype, transformers_version","float32, 4.10.0","None, 4.7.0.dev0"
tartuNLP,tartuNLP/EstBERT_NER_v2,tartuNLP/EstBERT_NER,Different,Different,"_name_or_path, classifier_dropout, id2label, label2id, position_embedding_type, torch_dtype, transformers_version, use_cache","tartuNLP/EstBERT_NER_v2, None, {'0': 'O', '1': 'B-PER', '2': 'I-PER', '3': 'B-ORG', '4': 'I-ORG', '5': 'B-LOC', '6': 'I-LOC', '7': 'B-GPE', '8': 'I-GPE', '9': 'B-PROD', '10': 'I-PROD', '11': 'B-TITLE', '12': 'I-TITLE', '13': 'B-EVENT', '14': 'I-EVENT', '15': 'B-DATE', '16': 'I-DATE', '17': 'B-TIME', '18': 'I-TIME', '19': 'B-MONEY', '20': 'I-MONEY', '21': 'B-PERCENT', '22': 'I-PERCENT'}, {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-GPE': 7, 'I-GPE': 8, 'B-PROD': 9, 'I-PROD': 10, 'B-TITLE': 11, 'I-TITLE': 12, 'B-EVENT': 13, 'I-EVENT': 14, 'B-DATE': 15, 'I-DATE': 16, 'B-TIME': 17, 'I-TIME': 18, 'B-MONEY': 19, 'I-MONEY': 20, 'B-PERCENT': 21, 'I-PERCENT': 22}, absolute, float32, 4.16.2, True","None, None, {'0': 'B-LOC', '1': 'B-ORG', '2': 'B-PER', '3': 'I-LOC', '4': 'I-ORG', '5': 'I-PER', '6': 'O'}, {'B-LOC': 0, 'B-ORG': 1, 'B-PER': 2, 'I-LOC': 3, 'I-ORG': 4, 'I-PER': 5, 'O': 6}, None, None, None, None"